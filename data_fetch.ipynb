{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f902057-65db-4040-85df-810295128572",
   "metadata": {},
   "source": [
    "# Extracting Reddit Information Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac13caa-69e5-4e09-9b7e-cfb8e3a422d8",
   "metadata": {},
   "source": [
    "### Code in this section is necessary to properly extract Post and Comment information from relevant state reddits\n",
    "**NOTE: It is not possible to extract the necessary information from 9 states due to the limits of reddit's API. It is not possible to extract the relevant posts from requested 3 week time periods before and after the election due to the amount of posts on these specific state subreddits. The states in question are: {'northcarolina', 'minnesota', 'newjersey', 'ohio', 'massachusetts', 'pennsylvania', 'wisconsin', 'texas', 'florida', 'connecticut'}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "747ae795-9cbb-4852-958e-015346e613e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import re\n",
    "from datetime import datetime, UTC\n",
    "import pytz\n",
    "import praw\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "003f02c3-1c75-4eba-8636-df0b231982cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['california', 'michigan', 'colorado', 'oregon', 'hawaii', 'oklahoma', 'maryland', 'arizona', 'virginia', 'maine', 'indiana', 'iowa', 'washington', 'newhampshire', 'alaska', 'louisiana', 'vermont', 'newyork', 'arkansas', 'alabama', 'kentucky', 'southcarolina', 'georgia', 'montana', 'delaware', 'utah', 'rhodeisland', 'missouri', 'tennessee', 'nebraska', 'illinois', 'westvirginia', 'newmexico', 'mississippi', 'kansas', 'northdakota', 'idaho', 'southdakota', 'wyoming', 'nevada']\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT SUBREDDIT NAMES FROM THE TEXT FILE\n",
    "state_subreddit_text = \"\"\n",
    "\n",
    "with open('state_reddits.txt', 'r') as file:\n",
    "    state_subreddit_text = file.read()\n",
    "\n",
    "text_list = re.split(' |\\n', state_subreddit_text)\n",
    "state_subreddits = [sub[2:] for sub in text_list if len(sub) > 0 and sub[0:2] == \"r/\"]\n",
    "\n",
    "# Delete invalid state subreddits \n",
    "invalid_states = {'northcarolina', 'minnesota', 'newjersey', 'ohio', 'massachusetts', 'pennsylvania', 'wisconsin', 'texas', 'florida', 'connecticut'}\n",
    "valid_state_subreddits = [state for state in state_subreddits if state not in invalid_states]\n",
    "print(valid_state_subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae268330-ddbe-4933-a961-aa82ee37c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Reddit Client using pre-made credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"P1pvruNSVTvfHuAPk9jIPw\",\n",
    "    client_secret=\"6x-bxO9xIUyyAv640iJpqkZxoSpkKA\",\n",
    "    refresh_token=\"2074775942901-SVxkdV43tApwU_FMAN4piRs-jlzXIQ\",\n",
    "    user_agent=\"python:US_Election:v1.0 (by u/Watermelon_boiii)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f92e2d9-b255-491c-a86f-4c3ea10a8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper variables and functions to extract post data\n",
    "\n",
    "# Define time cutoffs\n",
    "# Oct 15th 8pm EST\n",
    "before_elec_time = datetime(2024, 10, 15, 20, 0, 0, tzinfo=pytz.timezone('America/New_York')).astimezone(pytz.utc).timestamp()\n",
    "# Nov 5th 8pm EST\n",
    "election_time = datetime(2024, 11, 5, 20, 0, 0, tzinfo=pytz.timezone('America/New_York')).astimezone(pytz.utc).timestamp()\n",
    "# Nov 26th 8pm EST\n",
    "after_elec_time = datetime(2024, 11, 26, 20, 0, 0, tzinfo=pytz.timezone('America/New_York')).astimezone(pytz.utc).timestamp()\n",
    "\n",
    "\n",
    "def get_all_state_posts(reddit, states, date_cutoff):\n",
    "    all_posts = {}\n",
    "    for state in states:\n",
    "        posts = get_subreddit_posts(reddit, state, date_cutoff)\n",
    "        all_posts[state] = posts\n",
    "    return all_posts\n",
    "\n",
    "def get_subreddit_posts(reddit, state, date_cutoff):\n",
    "    posts = []\n",
    "    data = reddit.subreddit(state).new(limit=1000)\n",
    "    for raw_post in data:\n",
    "        post = vars(raw_post)\n",
    "        post_date = post[\"created_utc\"]\n",
    "        if post_date <= date_cutoff:\n",
    "            break\n",
    "        posts.append(post)\n",
    "    return posts\n",
    "\n",
    "def split_posts_by_date(posts, election_time):\n",
    "    before_posts = {}\n",
    "    after_posts = {}\n",
    "    for state in posts:\n",
    "        curr_after_posts = []\n",
    "        for post in posts[state]:\n",
    "            post_date = post[\"created_utc\"]\n",
    "            if post_date >= election_time:\n",
    "                curr_after_posts.append(post)\n",
    "            else:\n",
    "                break\n",
    "        after_posts[state] = curr_after_posts\n",
    "        before_posts[state] = posts[state][len(curr_after_posts):]\n",
    "    return before_posts, after_posts\n",
    "\n",
    "def write_posts_to_json(file_name, data):\n",
    "    for state in data:\n",
    "        for post in data[state]:\n",
    "            clean_post(post)\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def check_for_objects(value):\n",
    "    reddit_objects = {praw.models.Subreddit, praw.reddit.Reddit, praw.reddit.Redditor, praw.models.comment_forest.CommentForest, praw.models.Submission, praw.models.reddit.poll.PollData}    \n",
    "    for reddit_obj in reddit_objects:\n",
    "        if isinstance(value, reddit_obj):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_post(input_dict):\n",
    "    for key, value in input_dict.items():\n",
    "        # Check if the value is an instance of praw.reddit.Reddit or any other non-serializable object\n",
    "        if check_for_objects(value):\n",
    "            input_dict[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfe1436f-a4c0-4093-95f4-64aaf092b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute above functions, creating two json files, one for posts from before the election and one for posts after the election\n",
    "all_posts = get_all_state_posts(reddit, valid_state_subreddits, before_elec_time)\n",
    "before_elec_posts, after_elec_posts = split_posts_by_date(all_posts, election_time)\n",
    "write_posts_to_json(\"before_election_posts_data.json\", before_elec_posts)\n",
    "write_posts_to_json(\"after_election_posts_data.json\", after_elec_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53f8c8fd-2356-464d-b903-4a4a5116d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper variables and functions to extract comment data\n",
    "# WARNING: THIS IS A LOT OF DATA\n",
    "\n",
    "def get_comments_from_post(reddit, post):\n",
    "    url = \"https://www.reddit.com\" + post[\"permalink\"]\n",
    "    submission = reddit.submission(url=url)\n",
    "    submission.comments.replace_more(limit=0)  # Replace \"more comments\" to fetch all comments\n",
    "    comments = []\n",
    "    for raw_comment in submission.comments.list():\n",
    "        comment = vars(raw_comment)\n",
    "        comments.append(comment)\n",
    "    return comments\n",
    "\n",
    "def get_comments_from_subreddit(reddit, state_posts):\n",
    "    comments = {}\n",
    "    for post in state_posts:\n",
    "        comments[post[\"name\"]] = get_comments_from_post(reddit, post)\n",
    "    return comments\n",
    "\n",
    "def get_all_comments(reddit, posts_dict):\n",
    "    all_comments = {}\n",
    "    for state in posts_dict:\n",
    "        all_comments[state] = get_comments_from_subreddit(reddit, posts_dict[state])\n",
    "    return all_comments\n",
    "\n",
    "def write_comments_to_json(file_name, data):\n",
    "    for state in data:\n",
    "        for post_id in data[state]:\n",
    "            for comment in data[state][post_id]:\n",
    "                clean_comment(comment)\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def check_for_objects(value):\n",
    "    reddit_objects = {praw.models.Subreddit, praw.reddit.Reddit, praw.reddit.Redditor, praw.models.comment_forest.CommentForest, praw.models.Submission, praw.models.reddit.poll.PollData}    \n",
    "    for reddit_obj in reddit_objects:\n",
    "        if isinstance(value, reddit_obj):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_comment(input_dict):\n",
    "    for key, value in input_dict.items():\n",
    "        # Check if the value is an instance of praw.reddit.Reddit or any other non-serializable object\n",
    "        if check_for_objects(value):\n",
    "            input_dict[key] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "138f6736-12a0-4675-b31a-1917769caf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and save comments into a JSON file\n",
    "# NOTE: only run locally with states nevada and wyoming due to size of data. Use normal variables, not sample variables in actual run\n",
    "sample_before_elec_posts = {\"nevada\": before_elec_posts[\"nevada\"], \"wyoming\": before_elec_posts[\"wyoming\"]}\n",
    "sample_after_elec_posts = {\"nevada\": after_elec_posts[\"nevada\"], \"wyoming\": after_elec_posts[\"wyoming\"]}\n",
    "\n",
    "before_elec_comments = get_all_comments(reddit, sample_before_elec_posts)\n",
    "# before_elec_comments = get_all_comments(reddit, before_elec_posts)\n",
    "write_comments_to_json(\"before_election_comments_data.json\", before_elec_comments)\n",
    "after_elec_comments = get_all_comments(reddit, sample_after_elec_posts)\n",
    "# after_elec_comments = get_all_comments(reddit, after_elec_posts)\n",
    "write_comments_to_json(\"after_election_comments_data.json\", after_elec_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ab933-ba94-4324-8b91-4bfd790f70ff",
   "metadata": {},
   "source": [
    "# End of script\n",
    "\n",
    "The rest of the notebook is exploratory code used to help write this script. It is left commented out for now in case it is necessary in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f134ab24-6639-4778-8194-f919debd1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_posts = 1000\n",
    "# curr_cuttoffs, posts = find_posts_in_timeframe(reddit, list(states_to_find), num_posts)\n",
    "\n",
    "# for state, value in curr_cuttoffs.items():\n",
    "#     if value <= 900:\n",
    "#         continue\n",
    "#     else:\n",
    "#         del posts[state]\n",
    "#         states_to_find.remove(state)\n",
    "#         state_cutoffs[state] = value\n",
    "\n",
    "# invalid_states = states_to_find\n",
    "# print(state_cutoffs)\n",
    "# print(invalid_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdff29-d52b-409c-a9e1-6702ea84fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import praw\n",
    "# import time\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# # Initialize reddit client\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id=\"P1pvruNSVTvfHuAPk9jIPw\",\n",
    "#     client_secret=\"6x-bxO9xIUyyAv640iJpqkZxoSpkKA\",\n",
    "#     redirect_uri=\"http://localhost:8080\",\n",
    "#     user_agent=\"python:US_Election:v1.0 (by u/Watermelon_boiii)\",\n",
    "# )\n",
    "\n",
    "# # You need to obtain an authorization URL\n",
    "# auth_url = reddit.auth.url(scopes=['identity, read'], state=\"random\", duration='permanent')\n",
    "\n",
    "# # This URL will guide you to Reddit's OAuth page to authorize access\n",
    "# print(\"Visit this URL to authorize:\", auth_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90928adf-c9f2-4ebd-a38b-10569e7731d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code = \"HAWkqxUwIPFqk2hggjiUwLAvXbQVoA\"\n",
    "# print(reddit.auth.authorize(code))\n",
    "# print(reddit.user.me())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ee460-ccbd-45d1-9977-078571cb9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try using refresh token for reddit client\n",
    "# reddit = praw.Reddit(\n",
    "#     client_id=\"P1pvruNSVTvfHuAPk9jIPw\",\n",
    "#     client_secret=\"6x-bxO9xIUyyAv640iJpqkZxoSpkKA\",\n",
    "#     refresh_token=\"2074775942901-SVxkdV43tApwU_FMAN4piRs-jlzXIQ\",\n",
    "#     user_agent=\"python:US_Election:v1.0 (by u/Watermelon_boiii)\",\n",
    "# )\n",
    "# print(reddit.auth.scopes())\n",
    "\n",
    "# posts = reddit.subreddit(\"NewYork\").new(limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59dbcf-c3ff-4a6a-9dc1-8f551decc3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# def get_posts(reddit, state):\n",
    "#     posts = []\n",
    "#     data = reddit.subreddit(state).new(limit=50)\n",
    "#     for raw_post in data:\n",
    "#         post = vars(raw_post)\n",
    "#         if post_too_old(post):\n",
    "#             break\n",
    "#         url = \"https://www.reddit.com\" + post[\"permalink\"]\n",
    "#         # post[\"comments\"] = get_comments(reddit, url)\n",
    "#         posts.append(post)\n",
    "#     return posts\n",
    "\n",
    "# def post_too_old(post):\n",
    "#     # UTC Unix timestamp\n",
    "#     timestamp = post[\"created_utc\"]\n",
    "    \n",
    "#     # # Convert Unix timestamp to datetime\n",
    "#     # dt = datetime.utcfromtimestamp(timestamp)\n",
    "#     # print(\"UTC datetime:\", dt)\n",
    "\n",
    "#     # Get the current UTC time\n",
    "#     current_time = datetime.utcnow()\n",
    "    \n",
    "#     # Calculate the time 4 weeks ago\n",
    "#     four_weeks_ago = current_time - timedelta(weeks=4)\n",
    "    \n",
    "#     # Convert the post's timestamp to a datetime object\n",
    "#     post_time = datetime.utcfromtimestamp(timestamp)\n",
    "#     return post_time <= four_weeks_ago\n",
    "    \n",
    "\n",
    "# def get_comments(reddit, url): \n",
    "#     url = \"https://www.reddit.com\" + post[\"permalink\"]\n",
    "#     submission = reddit.submission(url=url)\n",
    "#     submission.comments.replace_more(limit=0)  # Replace \"more comments\" to fetch all comments\n",
    "    \n",
    "#     comments = []\n",
    "#     for raw_comment in submission.comments.list():\n",
    "#         comment = vars(raw_comment)\n",
    "#         comments.append(comment)\n",
    "\n",
    "#     return comments\n",
    "\n",
    "# # posts = get_posts(reddit, \"wyoming\")\n",
    "# # print(posts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9ab52-3629-41f5-9dbc-24faeec4b6d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # iterate over all state subreddits\n",
    "# state_posts = {}\n",
    "# for state in state_subreddits:\n",
    "#     posts = get_posts(reddit, state)\n",
    "#     state_posts[state] = posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99edd8c-44f5-43f3-b180-37eb482a17a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# # create json \n",
    "# filename = 'state_reddit_data.json'\n",
    "\n",
    "# def clean_post(input_dict):\n",
    "#     cleaned_dict = {}\n",
    "#     for key, value in input_dict.items():\n",
    "#         # Check if the value is an instance of praw.reddit.Reddit or any other non-serializable object\n",
    "#         if check_for_objects(value) or key == \"comments\":\n",
    "#             cleaned_dict[key] = None  # Replace Reddit object with None (or you could choose to remove it entirely)\n",
    "#         else:\n",
    "#             # Keep serializable values\n",
    "#             cleaned_dict[key] = value\n",
    "\n",
    "#     comments = []\n",
    "#     for comment in input_dict[\"comments\"]:\n",
    "#         comments.append(clean_comment(comment))\n",
    "#     cleaned_dict[\"comments\"] = comments\n",
    "        \n",
    "    \n",
    "#     return cleaned_dict\n",
    "\n",
    "# def clean_comment(comment):\n",
    "#     new_comment = {}\n",
    "#     for key, value in comment.items():\n",
    "#         if check_for_objects(value):\n",
    "#             # Skip adding this key-value pair if it's a Subreddit object\n",
    "#             continue\n",
    "#         else:\n",
    "#             # Keep all other values\n",
    "#             new_comment[key] = value\n",
    "#     return new_comment\n",
    "\n",
    "\n",
    "# def check_for_objects(value):\n",
    "#     reddit_objects = {praw.models.Subreddit, praw.reddit.Reddit, praw.reddit.Redditor, praw.models.comment_forest.CommentForest, praw.models.Submission, praw.models.reddit.poll.PollData}    \n",
    "#     for reddit_obj in reddit_objects:\n",
    "#         if isinstance(value, reddit_obj):\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "# # for state in state_posts:\n",
    "# #     for post in state:\n",
    "# #         clean_dict(post)\n",
    "\n",
    "# clean_state_posts = {}\n",
    "# for state in state_posts:\n",
    "#     post_list = []\n",
    "#     for post in state_posts[state]:\n",
    "#         new_dict = clean_post(post)\n",
    "#         post_list.append(new_dict)\n",
    "#     clean_state_posts[state] = post_list\n",
    "    \n",
    "# # Open the file in write mode and save the dictionary as JSON\n",
    "# with open(filename, 'w') as file:\n",
    "#     json.dump(clean_state_posts, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43c579-efb0-42f7-a59d-6ac0d8235290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Get comments off a post\n",
    "# post_url = \" https://www.reddit.com/r/newyork/comments/1gvveui/weirdly_specific_regional_request_trying_to_find/\"\n",
    "\n",
    "# submission = reddit.submission(url=post_url)\n",
    "\n",
    "# # Fetch the comments\n",
    "# submission.comments.replace_more(limit=0)  # Replace \"more comments\" to fetch all comments\n",
    "\n",
    "# # Print the comments\n",
    "# for comment in submission.comments.list():\n",
    "#     print(f\"Comment by {comment.author}: {comment.body}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e2c7e-4570-4379-b21f-aae975580589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Define the subreddit and the time frame (last two weeks)\n",
    "# subreddit = reddit.subreddit(\"AITA\")\n",
    "\n",
    "# # Get the current time and the time from two weeks ago\n",
    "# now = time.time()  # current time in seconds since epoch\n",
    "# two_weeks_ago = now - (2 * 7 * 24 * 60 * 60)  # 2 weeks in seconds\n",
    "\n",
    "# # Fetch new posts\n",
    "# posts = subreddit.new(limit=100)  # Fetch 100 most recent posts (you can adjust this)\n",
    "# filtered_posts = []\n",
    "\n",
    "# for post in posts:\n",
    "#     if post.created_utc >= two_weeks_ago:\n",
    "#         filtered_posts.append(post)\n",
    "\n",
    "# # Display the filtered posts\n",
    "# for post in filtered_posts:\n",
    "#     post_time = datetime.utcfromtimestamp(post.created_utc)\n",
    "#     print(f\"Title: {post.title}\")\n",
    "#     print(f\"Score: {post.score}\")\n",
    "#     print(f\"URL: {post.url}\")\n",
    "#     print(f\"Author: {post.author}\")\n",
    "#     print(f\"Created: {post_time}\")\n",
    "#     print(f\"Comments: {post.num_comments}\")\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ef0ab-ace9-4aa2-8a88-c021dad9636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import base64\n",
    "\n",
    "# # Define Reddit OAuth2 endpoint and your credentials\n",
    "# client_id=\"P1pvruNSVTvfHuAPk9jIPw\"\n",
    "# client_secret=\"6x-bxO9xIUyyAv640iJpqkZxoSpkKA\"\n",
    "# user_agent = \"python:US_Election:v1.0 (by u/Watermelon_boiii)\"\n",
    "\n",
    "# # The token URL for Reddit\n",
    "# token_url = \"https://www.reddit.com/api/v1/access_token\"\n",
    "\n",
    "# # Prepare the authentication headers and payload\n",
    "# auth = base64.b64encode(f\"{client_id}:{client_secret}\".encode(\"utf-8\")).decode(\"utf-8\")\n",
    "# headers = {\n",
    "#     \"User-Agent\": user_agent,\n",
    "#     \"Authorization\": f\"Basic {auth}\"\n",
    "# }\n",
    "\n",
    "# # Define the payload to request the Bearer token\n",
    "# payload = {\n",
    "#     \"grant_type\": \"client_credentials\",\n",
    "#     \"scope\": \"read\"\n",
    "# }\n",
    "\n",
    "# # Make the request to get the access token\n",
    "# response = requests.post(token_url, headers=headers, data=payload)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     # Extract the token from the response\n",
    "#     token_data = response.json()\n",
    "#     bearer_token = token_data[\"access_token\"]\n",
    "#     print(\"Bearer Token:\", bearer_token)\n",
    "# else:\n",
    "#     print(f\"Failed to get access token: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24de950-8328-4d5d-9880-e28ce04f7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from datetime import datetime, timedelta\n",
    "# import time\n",
    "\n",
    "# # Define Reddit API endpoint for new posts\n",
    "# url = \"https://api.reddit.com/r/newyork/new\"\n",
    "\n",
    "# # Define the time frame for the last two weeks\n",
    "# now = time.time()  # current time in seconds since epoch\n",
    "# two_weeks_ago = now - (2 * 7 * 24 * 60 * 60)  # 2 weeks in seconds\n",
    "\n",
    "# # Set parameters (you can adjust 'limit' as needed)\n",
    "# params = {\n",
    "#     \"limit\": 100,  # Number of posts to retrieve\n",
    "#     \"t\": \"all\"     # Retrieve posts from any time frame\n",
    "# }\n",
    "\n",
    "# # Add the OAuth2 access token to the headers\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "#     \"User-Agent\": \"python:US_Election:v1.0 (by u/Watermelon_boiii)\"\n",
    "# }\n",
    "\n",
    "# # Make the GET request\n",
    "# response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# # Check if the request was successful\n",
    "# if response.status_code == 200:\n",
    "#     data = response.json()\n",
    "\n",
    "#     # Filter posts to get only those from the last two weeks\n",
    "#     filtered_posts = [\n",
    "#         post['data'] for post in data['data']['children']\n",
    "#         if post['data']['created_utc'] >= two_weeks_ago\n",
    "#     ]\n",
    "    \n",
    "#     # Display the filtered posts\n",
    "#     for post_data in filtered_posts:\n",
    "#         title = post_data['title']\n",
    "#         score = post_data['score']\n",
    "#         post_url = post_data['url']\n",
    "#         author = post_data['author']\n",
    "#         created_time = datetime.utcfromtimestamp(post_data['created_utc'])\n",
    "#         num_comments = post_data['num_comments']\n",
    "\n",
    "#         print(f\"Title: {title}\")\n",
    "#         print(f\"Score: {score}\")\n",
    "#         print(f\"URL: {post_url}\")\n",
    "#         print(f\"Author: {author}\")\n",
    "#         print(f\"Created: {created_time}\")\n",
    "#         print(f\"Comments: {num_comments}\")\n",
    "#         print(\"-\" * 80)\n",
    "# else:\n",
    "#     print(f\"Failed to fetch posts: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
